% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/broadcasted_sparsetenreg.R
\name{broadcasted_sparsetenreg}
\alias{broadcasted_sparsetenreg}
\title{Fit a broadcasted nonparametric tensor regression model with elastic net regularization}
\usage{
broadcasted_sparsetenreg(
  X_sample,
  y,
  r = 2,
  lambda = 0,
  alpha = 0.5,
  gamma = 0,
  alpha_gamma = 0,
  warmstart = 1,
  beta0 = NA,
  B0 = NA,
  BB0 = NA,
  Replicates = 1,
  QCQP = 1,
  epsilon = 1e-07,
  MaxIter = 10000,
  TolFun = 1e-04,
  rescale_l2 = 1,
  rescale_all = 0,
  Memorysave = 0,
  manifold = 0,
  family = "gaussian",
  Z_sample = NA,
  improvement = 1,
  stoppingrule_penalty = 1,
  shrink_factor_number = 5,
  startsize = 3,
  restriction = 0,
  knots = 0,
  penalty = c("L1L2")
)
}
\arguments{
\item{X_sample}{The training input with basis transformation; the tensor of size \eqn{ p_1 \times \cdots \times p_D \times (K-1) \times n}}

\item{y}{The training output. a vector of size \eqn{n}}

\item{r}{CP rank used in the algorithm}

\item{lambda}{tuning parameters: \eqn{\lambda_1} in the paper}

\item{alpha}{tuning parameters: \eqn{\lambda_2} in the paper}

\item{gamma}{the tuning parameter for the basis components in CP decomposition (equivalent to \eqn{\lambda_1} of the elastic net)}

\item{alpha_gamma}{the tuning parameter for the basis components in CP decomposition (equivalent to \eqn{\lambda_2} of the elastic net)}

\item{warmstart}{warmstart strategy; 1 is sequential; 3 is one time.}

\item{beta0}{initial intercept term}

\item{B0}{ipitial coefficient (CP decomposition form) for the covariate tensor}

\item{BB0}{initial coefficient (tensor form) for the covariate tensor}

\item{Replicates}{number of initial points used by the algorithm}

\item{QCQP}{update the basis components by QCQP optimization (1) or not (0)}

\item{epsilon}{tolerance value in block update}

\item{MaxIter}{maximum of the outer iteration number}

\item{TolFun}{tolerance value of the outer iteration (the whole algorithm)}

\item{rescale_l2}{initial rescale strategy: l_2 method (1) or l_1 method (0).}

\item{rescale_all}{rescale all CP components including the coefficient of the basis (1) or without the coeffcients of basis (0)}

\item{Memorysave}{run the algorithm with saving (1) or withougt saving the memory (0)}

\item{manifold}{used in the future}

\item{family}{used in the future}

\item{Z_sample}{used in the future}

\item{improvement}{numerical stablization improvementation (1) or not (0)}

\item{stoppingrule_penalty}{stoppoing rule. 1 means that the objective function is loss + penalty; 0 means that the objective function is just loss.}

\item{shrink_factor_number}{number of sequential initialization}

\item{startsize}{the minimal down-size in the sequential initialization}

\item{restriction}{used in the future}

\item{knots}{used in the future}

\item{penalty}{used in the future}
}
\value{
\item{beta0}{the estimated intercept term}
\item{beta}{the estimated coefficient tensor (in CP decomposition form)}
\item{ob_f_loss_penalty_final}{the final objective function value track}
\item{others}{used for the package developer}
}
\description{
A scale-adjusted block-wise descent algorithm to solve the optimization based on Broadcasted Nonparametric Tensor Regression model
}
\examples{
# load the true coefficient tensor (matrix)
data("X_horse")
BB <- X_horse

set.seed(2019)

# sample size of the training and test set
n_train <- 400
n_test <- 100

# signal level
signal_level = 0.5


# the tuning parameters have been tuned by authors though validation; see the paper and test_validation.R
# rank used in the algorithm
rank <- 4
# the tuning parameters of the elastic net
lambda_1 <- 5
lambda_2 <- 0.5  # 0 corresponds to lasso; 1 corresponds to ridge
# use the initial point tuned from the validation (1) or not (0, sequential warmstart)
input_initial <- 0


# generate the training data
X_train = array(runif(prod(c(dim(BB), n_train)), 0, 1), c(dim(BB), n_train))
# broadcated procedure
BroX_train = X_train + 0.6 * sin(2 * pi * (X_train - 0.5)^2)
y_train = 1 + crossprod(matrix(BroX_train, c(prod(dim(BB)), n_train)), as.vector(BB)) + signal_level * rnorm(n_train)


# generate the test data
X_test = array(runif(prod(c(dim(BB), n_test)), 0, 1), c(dim(BB), n_test))
# broadcated procedure
BroX_test = X_test + 0.6 * sin(2 * pi * (X_test - 0.5)^2)
y_test = 1 + crossprod(matrix(BroX_test, c(prod(dim(BB)), n_test)), as.vector(BB)) + signal_level * rnorm(n_test)


# Transform to truncated power basis
num_knots <- 5
knots = quantile(c(X_train), probs = c(seq(0, 1, 1/(num_knots - 1))))
tildePhiX_train = tildePhiX_trans(X_train, knots)



# BroadcasTR
if (input_initial == 1) {
 data("initial_point")
 beta0 <- initial_point$beta0
 B0 <- initial_point$B0
 warmstart <- 0
} else {
beta0 <- NA
 B0 <- NA
 warmstart <- 1
}
res = broadcasted_sparsetenreg(tildePhiX_train, y_train, r = rank, lambda = lambda_1, alpha = lambda_2, warmstart = warmstart, beta0 = beta0, B0=B0, Replicates=1)

# norm tensor
normtensor <- fhatnorm_ten(full_R(res$beta), knots)

# plot the true and estimated region
par(mar = c(0, 0, 0, 0), mfcol = c(1, 2), mai = c(0.2, 0.4, 0.4, 0.2))
plot(c(1, dim(BB)[1]), c(1, dim(BB)[2]), xaxt = "n", yaxt = "n", type = "n")
rasterImage(BB, 0, 0, dim(BB)[1] + 1, dim(BB)[2] + 1)
mtext("True coefficient tensor (matrix)", 3, line = 0.2)
plot(c(1, dim(BB)[1]), c(1, dim(BB)[2]), xaxt = "n", yaxt = "n", type = "n")
mtext("Estimated norm tensor (matrix)", 3, line = 0.2)
rasterImage(normtensor, 0, 0, dim(BB)[1] + 1, dim(BB)[2] + 1)


# prediction on the test set
tildePhiX_test = tildePhiX_trans(X_test, knots)
y_pre = res$beta0 + crossprod(matrix(tildePhiX_test, c(prod(dim(full_R(res$beta))), n_test)), as.vector(full_R(res$beta)))



# prediction performance
cat("The prediction performance in these tunning parameters: \n", "MSPE =", sum((y_test - y_pre)^2)/n_test, "\n")

}
\references{
Y. Zhou, R. K. W. Wong and K. He. Broadcasted Nonparametric Tensor Regression
}
\seealso{
validation_broadcasted_sparsetenreg, sequential_warmstart
}
